{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d17959c0d998eca7b05aa116884954eea1ad8cbc"
   },
   "source": [
    "# Forecasting 3 Months of Sales\nGiven 5 years of daily sales data across 10 stores for 50 items, we have been tasked to forecast the next 3 months of sales. We will be exploring the data using Pandas and building models using ARIMA, tensorflow's DNN regressor, and xgboost.\n\nLet's get started!\n##### NOTE\nThis is my first competition and I'm still learning the models myself. At the end I share what I learned while building this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ff5bdb791aa944363c2566d7c72f7c4c60fda90"
   },
   "source": [
    "# Import Libraries\n",
    "Below are all the libraries that we'll use (with some extra for notebook aesthetics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "00f01634cd5de5d23a5f78b3a35fbcc18878e452"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from jupyterthemes import jtplot\n",
    "#jtplot.style(theme='chesterish')\n",
    "\n",
    "from scipy.spatial.distance import euclidean #used for fdt\n",
    "import fastdtw as fdt #fast dynamic time warping\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose #decompose seasonality\n",
    "from statsmodels.tsa.stattools import adfuller #test if series is stationary (then can perform ARIMA)\n",
    "\n",
    "\"\"\"from pyramid.arima import auto_arima #auto ARIMA model (pip install pyramid-arima)\"\"\"\n",
    "import xgboost as xgb #xgboost model\n",
    "import tensorflow as tf #DNN estimator model\n",
    "\n",
    "path = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43c339d866a5abf222af3a883f52a8f9faa6472b"
   },
   "source": [
    "# Metrics and 2 of the Models\n## Error Metric\nWe'll be using the Symmetric Mean Absolute Percentage Error as our forecasting error metric. Defining a function saves us from writing the code multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "bce0f7b6c63498fd2c3fa190d407b4ed79fbbe3f"
   },
   "outputs": [],
   "source": [
    "def SMAPE (forecast, actual):\n",
    "    \"\"\"Returns the Symmetric Mean Absolute Percentage Error between two Series\"\"\"\n",
    "    masked_arr = ~((forecast==0)&(actual==0))\n",
    "    diff = abs(forecast[masked_arr] - actual[masked_arr])\n",
    "    avg = (abs(forecast[masked_arr]) + abs(actual[masked_arr]))/2\n",
    "    \n",
    "    print('SMAPE Error Score: ' + str(round(sum(diff/avg)/len(forecast) * 100, 2)) + ' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7012503ab06070ef0dd013f2f4669275190d2af"
   },
   "source": [
    "## Stationarity Test (Dickey Fuller)\nTime Series data should be stationary before applying an ARIMA model. Stationary means that the mean, standard deviation, and variance don't change over time. The function below tests whether or not a Time Series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "b11df16c3ee4575b602102166850a33ee41e2757"
   },
   "outputs": [],
   "source": [
    "def Fuller(TimeSeries):\n",
    "    \"\"\"Provides Fuller test results for TimeSeries\"\"\"\n",
    "    stationary_test = adfuller(TimeSeries)\n",
    "    print('ADF Statistic: %f' % stationary_test[0])\n",
    "    print('p-value: %f' % stationary_test[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in stationary_test[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "033705460b9f85c2f262e5aaee87049144e4a153"
   },
   "source": [
    "## ARIMA Model\nGeneral ARIMA model that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "c3e25114229529a4a033a0d4187fe14fc057f893"
   },
   "outputs": [],
   "source": [
    "#def ARIMA(TimeSeries, maxP, maxQ, maxD):\n",
    "    \"\"\"Returns ARIMA model (not fitted)\"\"\"\n",
    "\"\"\"    stepwise_model = auto_arima(TimeSeries, start_p=1, start_q=1,\n",
    "                           max_p=maxP, max_q=maxQ,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, max_d=maxD, D=1, trace=False,\n",
    "                           error_action='ignore',\n",
    "                           suppress_warnings=True,\n",
    "                           stepwise=True,\n",
    "                           maxiter=500)\n",
    "    print(stepwise_model.aic())\n",
    "    return stepwise_model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b25e794ba4f585d3d58bf5047adbd22e700f647d"
   },
   "source": [
    "## XGBoost Model\nGeneral xgboost model that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "ee5898566f1d94084ae80cadbc814b4490b7dfc2"
   },
   "outputs": [],
   "source": [
    "def xboost(x_train, y_train, x_test):\n",
    "    \"\"\"Trains xgboost model and returns Series of predictions for x_test\"\"\"\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train, feature_names=list(x_train.columns))\n",
    "    dtest = xgb.DMatrix(x_test, feature_names=list(x_test.columns))\n",
    "\n",
    "    params = {'max_depth':3,\n",
    "              'eta':0.2,\n",
    "              'silent':1,\n",
    "              'subsample':1}\n",
    "    num_rounds = 1500\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_rounds)\n",
    "    \n",
    "    return pd.Series(bst.predict(dtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26125eb3d88b94b089484269169951a27700d4de"
   },
   "source": [
    "# Data Exploration\n## Retrieve Data\nOpen the competition training data. We'll be exploring this before splitting for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "aab1e27f272eadb49de1fea034e604a68f0d0fdd"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+'train.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "316fe276898b38b0ab69602384fa491ca9ece7ff"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "407f11b90921d705f90f7f59f3b9f995b62fd604"
   },
   "source": [
    "## Store Trends\nHere we're looking to see if there are any seasonality trends in the total store sales. We'll group by week so we can more clearly see trends in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "d66d775a195308f314b4051be0e5af077d7c289c"
   },
   "outputs": [],
   "source": [
    "stores = pd.DataFrame(df.groupby(['date','store']).sum()['sales']).unstack()\n",
    "stores = stores.resample('7D',label='left').sum()\n",
    "stores.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "8b52cd35ad97be23984812361edb8cf568a3f04e"
   },
   "outputs": [],
   "source": [
    "stores.plot(figsize=(16,9), title='Weekly Store Sales', legend=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2e70304872af04431caf2e765da2d0961d665fe"
   },
   "source": [
    "The above plot charts every store's sales by week. But how does the average trend? The 25% quartile?\n\nLet's look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "5120d692e6c9d85d86a21668745fedc227c7083e"
   },
   "outputs": [],
   "source": [
    "store_qtr = pd.DataFrame(stores.quantile([0.0,0.25,0.5,0.75,1.0],axis=1)).transpose()\n",
    "store_qtr.sort_index(inplace = True)\n",
    "store_qtr.columns = ['Min','25%','50%','75%','Max']\n",
    "store_qtr.plot(figsize=(16,9), title='Weekly Quartile Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5a10c793bbad7b3cd1be5a4f85a1269b4503f68"
   },
   "source": [
    "We can see there's quite a gap between the 25% quartile and average. However, as the other chart shows as well, each store shares a general seasonality. They have highs and lows during the same periods of time.\n\nLet's take a look at the seasonality aspect of the average. But before that, we're going to track the week-to-week difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "923cc64e00b41fe21c8f561122d04e94e81a1ed9"
   },
   "outputs": [],
   "source": [
    "seasonal = seasonal_decompose(pd.DataFrame(store_qtr['50%']).diff(1).iloc[1:,0],model='additive')\n",
    "seasonal.plot()\n",
    "plt.suptitle = 'Additive Seasonal Decomposition of Average Store Week-to-Week Sales'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "3259033cf5c9c59f1e2ea5a1d3303993466363a3"
   },
   "outputs": [],
   "source": [
    "Fuller(pd.DataFrame(store_qtr['50%']).diff(1).iloc[1:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fc188fd6586b6a28d7b96cfb08d2f419de58030"
   },
   "source": [
    "### Store Trends Conclusion\nThere is definitely seasonality in the store sales. Taking the week-to-week difference provides a dataset that is very likely to be stationary (< 1% chance that it's not). If we were to use this as a starting point for our model, we could cluster the stores to the nearest 25% quartile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a07376eea71fd221cd06bcbe3f9f7677e381bc8a"
   },
   "source": [
    "## Item Sales Trends\nNow we'll do the same analysis for the total item sales. And again, we're looking at weekly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "778c93bc4028866a508dfb491d32881bf6c70960"
   },
   "outputs": [],
   "source": [
    "items = pd.DataFrame(df.groupby(['date','item']).sum()['sales']).unstack()\n",
    "items = items.resample('7D',label='left').sum()\n",
    "items.sort_index(inplace = True)\n",
    "\n",
    "items.tail(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e82719d8ea0360718736e5a1a0474f01c4ffbaa7"
   },
   "outputs": [],
   "source": [
    "items.plot(figsize=(16,9), title='Weekly Item Sales', legend=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "215d8b90dc786d227f672b6f425ed1ad30da5584"
   },
   "source": [
    "Since there are more items than there were stores, we can look at more quartiles. Let's see how every 10% quartile trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "baaf58d06e6891af73819734b27e55864b67ba1c"
   },
   "outputs": [],
   "source": [
    "item_WK_qtr = pd.DataFrame(items.quantile([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],axis=1)).transpose()\n",
    "item_WK_qtr.sort_index(inplace = True)\n",
    "item_WK_qtr.columns = ['Min','10%','20%','30%','40%','50%','60%','70%','80%','90%','Max']\n",
    "item_WK_qtr.plot(figsize=(16,9), title='Weekly Quartile Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6c8a3db4f7cb10ef341ed3561e9424a7d1dee60"
   },
   "source": [
    "Like we saw in the store sales plots, there is seasonality in item sales. Let's break out the seasonal component for the average like we had before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "1badc1fb126eda457d3b1063564208fec522cd73"
   },
   "outputs": [],
   "source": [
    "seasonal = seasonal_decompose(pd.DataFrame(item_WK_qtr['50%']).diff(1).iloc[1:,0],model='additive')\n",
    "seasonal.plot()\n",
    "plt.title = 'Additive Seasonal Decomposition of Average Item Week-to-Week Sales'\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "16e23ced1e18982aa7826841ea3e541b391c3d7a"
   },
   "outputs": [],
   "source": [
    "Fuller(pd.DataFrame(item_WK_qtr['50%']).diff(1).iloc[1:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf78aae75a475373c2f6f500c9e478599d7a63e0"
   },
   "source": [
    "### Item Trend Conclusion\nItem sales are also seasonal. No surprise there. Week-to-week differencing provides a dataset that is very likely to be stationary (< 1% chance that it's not). If we were to use this as a basis for our model, we could cluster the items to the nearest 10% quartiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7487e5b523fcec7ae46eabdaf7b9d4d4ca50048a"
   },
   "source": [
    "## Store & Item Variability\nWe've seen how stores and items trend by themselves, but do some stores sell more of one item? In other words: do the stores have the same sales mix? Are the items sold evenly (percentage-wise) across all stores?\n\nBelow is a plot for the % distribution of each item's sales across the stores (each row adds to 100%). As we can see, it's very uniform. The takeaway here is that the items are sold evenly across the stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "eea6bd85565cec5bd758b4c4d8cda7d3b2a9e2a9"
   },
   "outputs": [],
   "source": [
    "store_item = df.groupby(by=['item','store']).sum()['sales'].groupby(level=0).apply(\n",
    "    lambda x: 100* x/ x.sum()).unstack()\n",
    "sns.heatmap(store_item, cmap='Blues', linewidths=0.01, linecolor='gray').set_title(\n",
    "    'Store % of Total Sales by Item')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5b9e074903aa3afbc972599bcfd29566ce9372e"
   },
   "source": [
    "Now to confirm, let's look at the % distribution of each store's sales across the different items (each row adds to 100%).\n\nWe can see that each store overall sold roughly the same percentage of each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "a2491da4d9a35e9dfff7c415f2bf51d5498866ba"
   },
   "outputs": [],
   "source": [
    "item_store = df.groupby(by=['store','item']).sum()['sales'].groupby(level=0).apply(\n",
    "    lambda x: 100* x/ x.sum()).unstack()\n",
    "sns.heatmap(item_store , cmap='Blues', linewidths=0.01, linecolor='gray').set_title(\n",
    "    'Item % of Total Sales by Store')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdfc977f16bc59d044c1834d8ffd8b24778df66e"
   },
   "source": [
    "### Store vs Item Conclusion\nItems have roughly same percentage sales across all stores. We could use this in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28cd6f6351c68c12632dcfa5e6cec33f2d1886a8"
   },
   "source": [
    "## Day of Week Variability\nHow do sales vary by day of week? Is there seasonality as well? Do stores share same trends? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "f61800dd5a4ef680d09179c3f0a2fc029e0b06b2"
   },
   "outputs": [],
   "source": [
    "df['Day'] = df.index.weekday_name\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "138f90da00013c8fc6102b8d8b58fe1a9e66900d"
   },
   "outputs": [],
   "source": [
    "dow_store = df.groupby(['store','Day']).sum()['sales'].groupby(level=0).apply(\n",
    "    lambda x: 100* x/ x.sum()).unstack().loc[:,['Monday',\n",
    "                                                'Tuesday',\n",
    "                                                'Wednesday',\n",
    "                                                'Thursday',\n",
    "                                                'Friday',\n",
    "                                                'Saturday',\n",
    "                                                'Sunday']]\n",
    "sns.heatmap(dow_store, cmap='Blues', linewidths=0.01, linecolor='gray').set_title(\n",
    "    'Day % of Total Sales by Store')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "093e1eea261e4315a3380b65fab47d7d6d8b3731"
   },
   "source": [
    "The plot above shows the % mix of store sales by day. We can see that the stores are very similar in what days are popular.\n\nLet's do the same for the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "86928ada1bb030a32dd93c88aacd374f110e019b"
   },
   "outputs": [],
   "source": [
    "dow_item = df.groupby(['item','Day']).sum()['sales'].groupby(level=0).apply(\n",
    "    lambda x: 100* x/ x.sum()).unstack().loc[:,['Monday',\n",
    "                                                'Tuesday',\n",
    "                                                'Wednesday',\n",
    "                                                'Thursday',\n",
    "                                                'Friday',\n",
    "                                                'Saturday',\n",
    "                                                'Sunday']]\n",
    "sns.heatmap(dow_item, cmap='Blues', linewidths=0.01, linecolor='gray').set_title(\n",
    "    'Day % of Total Sales by Item')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64078893ed8e5b9e23ae1165d2877fbc492b7046"
   },
   "source": [
    "This plot tells us that each item's sales are nearly identical in terms of which days are more popular.\n\nNow let's see if each day generally trends the same as the total week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "1a7ab63133e0597b2b7a89bfb7bedb8d63935021"
   },
   "outputs": [],
   "source": [
    "dow = pd.DataFrame(df.groupby(['date','Day']).sum()['sales']).unstack()['sales'].loc[:,\n",
    "                                                                                ['Monday',\n",
    "                                                                               'Tuesday',\n",
    "                                                                               'Wednesday',\n",
    "                                                                               'Thursday',\n",
    "                                                                               'Friday',\n",
    "                                                                               'Saturday',\n",
    "                                                                               'Sunday']]\n",
    "dow = dow.resample('7D',label='left').sum()\n",
    "dow.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e37c2151ea9d0e283d4555425cecf16222815030"
   },
   "outputs": [],
   "source": [
    "dow.plot(figsize=(16,9), title='Sales by Day of Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a2e4469b643bd92ab22b085d6b5f4aa58d19665"
   },
   "source": [
    "### Day of Week Conclusion\nDay of week does impact sales, however all stores & items have similar distributions. Day of week trends follow general weekly trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa731aba9c9b8e2f76505fa6a460ccadfa8bc3fe"
   },
   "source": [
    "## Findings and Steps Forward\nItems and stores weekly sales have seasonality and can be munged into a stationary dataset. They also have similar day of week variability, and items have roughly same distributions in stores.\n\n### Modeling Process\nSplit the data into train and test data (3 months of test). Will compare several models, all of which are outlined below. The goal is to find the model with the best accuracy.\n#### Model (1.1)\n+ Dynamic Time Warping (DTW) on item *__weekly__* sales to cluster to nearest 10% quartile\n+ Forecast with *__ARIMA__*\n+ Percentages will be used to find item sales by store by day\n\n#### Model (1.2)\n+ Forecast weekly item sales with *__ARIMA__*\n+ Percentages will be used to find item sales by store by day\n\n#### Model (2)\n+ Item *__daily__* sales with added features:\n + Day of year (in mod 364)\n + Day of week (numeric)\n + Month\n + Year\n + Prior year sales\n + Whether or not a weekend (Fri-Sun)\n + Dynamic Time Warping (DTW) on item weekly sales to cluster to nearest 10% quartile\n+ Forecast with *__feed forward neural network__*\n\n#### Model (3)\n+ Item *__daily__* sales with added features:\n + Day of year (in mod 364)\n + Day of week (numeric)\n + Month\n + Year\n + Prior year sales\n + Whether or not a weekend (Fri-Sun)\n + Dynamic Time Warping (DTW) on item weekly sales to cluster to nearest 10% quartile\n+ Forecast with *__xgboost__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cf15b622a19c8fff595212f8cff3cd9f20c1471"
   },
   "source": [
    "# ARIMA Models\n\nNOTE: Most of the ARIMA model code is commented due to Kaggle only allowing one custom library. Error results are reported at the end of each model.\n## Model (1.1) - Clustered Weekly Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "137f854a2e5050d2ed749bf571624faad44b8025"
   },
   "source": [
    "Will be using the 10% quartile weekly item sales that was created during the exploratory analysis. Since the competition is predicting the next 3 months of sales, we will use 3 months (13 weeks) of test data.\n\nWe will build an ARIMA model for each quartile then use clustering and percentages to arrive at daily items sales by store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e7109cc7bcc005014b0849e53da4813ae7e470fb"
   },
   "outputs": [],
   "source": [
    "train = item_WK_qtr[:-13]\n",
    "test = df.loc[df.index >= pd.to_datetime('October 3, 2017')] # last 13 weeks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e557372f281baebbca1e598762a17be44d656c49"
   },
   "outputs": [],
   "source": [
    "store_pct = store_item.transpose()\n",
    "store_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0174d28394e5ce992896d72853b8c6ffb64b08dd"
   },
   "source": [
    "#### Dynamic Time Warping to 10% Quartiles\nMatches each item to nearest 10% quartile. Outputs list of item id, % quartile /10, and dtw score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "2ab0bd8545d9316d09d1a70464623955e8ba8ad6"
   },
   "outputs": [],
   "source": [
    "fitted_items_WK = []\n",
    "qtr_list = [0] *11\n",
    "\n",
    "for column in items:\n",
    "    for c in range(11):\n",
    "        qtr_list[c] = [fdt.fastdtw(items[column],item_WK_qtr.iloc[:,c], dist= euclidean)[0], c]\n",
    "    qtr_list.sort()\n",
    "    fitted_items_WK.append([column[1], qtr_list[0][1], qtr_list[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d73508e13883efb0939cceb8912c95b097e8b8a3"
   },
   "source": [
    "#### Fitting Models and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e5c9dd2ee2ab6ce01f04946b489dc0b097fd9eb1"
   },
   "outputs": [],
   "source": [
    "\"\"\"ARIMA_predictions = pd.DataFrame()\n",
    "\n",
    "for column in item_WK_qtr:\n",
    "    model = ARIMA(item_WK_qtr[column], 52, 52, 52)\n",
    "    model.fit(train[column])\n",
    "    ARIMA_predictions[column] = model.predict(n_periods=13)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ca4b14da8af36306244a4c09da950b48fed4fbd"
   },
   "source": [
    "Use item quartile fittings to assign each item a forecast from ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "d86a8b4e0b072bd13ac40a8ee1de30baf5e6ad7b"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_WK_predictions = pd.DataFrame()\n",
    "\n",
    "for i in range(50):\n",
    "    item_WK_predictions[fitted_items_WK[i][0]] = ARIMA_predictions.iloc[:,fitted_items_WK[i][1]]\n",
    "\n",
    "item_WK_predictions.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e8ef5ce46fb64ed4f81e20b23ea48300aa15c8b"
   },
   "source": [
    "#### Convert Item Weekly Predictions to Daily Predictions\nUse day of week percentages from before to calculate daily item sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "d149c9d7c24b17b29072d073a58fcedd141e52cf"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_pred = []\n",
    "\n",
    "for column in item_WK_predictions:\n",
    "    for i, row in item_WK_predictions.iterrows():\n",
    "        for col in range(7):\n",
    "            item_Day_pred.append([i, dow_item.columns[col], column, dow_item.iloc[int(column)-1,col]\n",
    "                                 * item_WK_predictions[column][i]/100])\n",
    "            \n",
    "item_Day_fcst = pd.DataFrame(item_Day_pred, columns=['Week #','Day','item','Prediction'])\n",
    "\n",
    "item_Day_fcst.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8224bba275ac21dc39fbe4560173005ad8d8b393"
   },
   "source": [
    "#### Split Predictions by Store\nReshape the store_item DataFrame and use percentages to calculate daily item sales by store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "d497537c70350d6f2ce97d6a80f458de4bb80d51"
   },
   "outputs": [],
   "source": [
    "\"\"\"store_item = pd.DataFrame(store_item.stack()).reset_index()\n",
    "store_item.columns = ['item','store','pct']\n",
    "\n",
    "item_Day_fcst = item_Day_fcst.merge(store_item, on= 'item')\n",
    "\n",
    "item_Day_fcst['sales'] = item_Day_fcst['Prediction'] * item_Day_fcst['pct']/100\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "17e0a5da54391154f4eb53bce6c28006c158fc3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst = item_Day_fcst.loc[:,['Week #','Day','store','item','sales']]\n",
    "\n",
    "item_Day_fcst.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0fbc19b4b77d7c68b5d6b5d927aee6e3fd12511f"
   },
   "source": [
    "#### Convert Week Number and Day of Week into Datetime\nBased on where the data was split for testing, the weeks start on Tuesdays so there's no offset then. This adds an additional day of data that we'll need to cutoff.\n\nThis is needed so we can remove the additional day in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "ebfe14d531ae88047684b90fb85ab0d96255ee60"
   },
   "outputs": [],
   "source": [
    "#def str_to_date(row):\n",
    "    \"\"\"Takes day of week string and week offset to calculate date\"\"\"\n",
    "    \"\"\"switcher = {\n",
    "        'Tuesday': 0, #data starts on a Tuesday, so 0 offset\n",
    "        'Wednesday': 1,\n",
    "        'Thursday': 2,\n",
    "        'Friday': 3,\n",
    "        'Saturday': 4,\n",
    "        'Sunday': 5,\n",
    "        'Monday': 6\n",
    "    }\n",
    "    weeks = pd.to_timedelta(7* row['Week #'], unit='D')\n",
    "    days = pd.to_timedelta(switcher.get(row['Day']), unit='D')\n",
    "    \n",
    "    return pd.to_datetime('October 3, 2017') + weeks + days\n",
    "\n",
    "\n",
    "item_Day_fcst['Date'] = item_Day_fcst.apply(lambda row: str_to_date(row), axis=1)\n",
    "item_Day_fcst.index = item_Day_fcst['Date']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "a6bd49e40df3fe43434ff6e094ed3c6a2f9fb234"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst.sort_values(['item','store','Date'], inplace=True)\n",
    "item_Day_fcst['sales']= round(item_Day_fcst['sales'], 0)\n",
    "\n",
    "item_Day_fcst = item_Day_fcst[['store','item','sales']].loc[\n",
    "    item_Day_fcst.index < pd.to_datetime('January 1, 2018')]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f528bed04d02ebe05448c9a932aa1c64256ae8f"
   },
   "source": [
    "#### Model Accuracy\nThe predictions have been organized the same as the testing data, so we can simply plug both into our error function.\n\nFrom this model we get 19.49% error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "f771bef59ceef862505cdd0a9b3882aed32d1aa3"
   },
   "outputs": [],
   "source": [
    "\"\"\"SMAPE(item_Day_fcst['sales'], test['sales'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f6b5c750b7024483d534d799f9c9d172d511b39"
   },
   "source": [
    "## Model (1.2) - Unclustered Weekly Data\n\nNow that we've forecasted item quartiles, let's forecast for each item separately. This is to see if there's a difference in accuracy.\n\nWe will build an ARIMA model for each item then use percentages to arrive at daily items sales by store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e4b7111c400823cf4e9aad751c709695c2720e8e"
   },
   "outputs": [],
   "source": [
    "train = items['sales'][:-13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "539d5c2cf444c814a551b3beedd22891ef0d4b0f"
   },
   "source": [
    "#### Fitting Models and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "f8178c6b3ca19243f0cf64c7df7c57785ab8ff9c"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_WK_predictions = pd.DataFrame()\n",
    "\n",
    "for column in items['sales']:\n",
    "    model = ARIMA(items['sales'][column], 52, 52, 52)\n",
    "    model.fit(train[column])\n",
    "    item_WK_predictions[column] = model.predict(n_periods=13)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ece67fc8f5bebff0f41cf1cd4d3846aee0f0b206"
   },
   "source": [
    "#### Convert Item Weekly Predictions to Daily Predictions\nUsing day of week percentages from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "8987a08a252647546ee41b66092a5049fa9c04c9"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_pred = []\n",
    "\n",
    "for column in item_WK_predictions:\n",
    "    for i, row in item_WK_predictions.iterrows():\n",
    "        for col in range(7):\n",
    "            item_Day_pred.append([i, dow_item.columns[col], column, dow_item.iloc[int(column)-1,col]\n",
    "                                 * item_WK_predictions[column][i]/100])\n",
    "            \n",
    "item_Day_fcst = pd.DataFrame(item_Day_pred, columns=['Week #','Day','item','Prediction'])\n",
    "\n",
    "item_Day_fcst.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "531988a8e5fc2c944b4da3c54a54ca71fe5c5b4b"
   },
   "source": [
    "#### Split Predictions by Store\nReshape the store_item DataFrame and use percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "8b14d710fa6b98f3942d7e3ce205febc5b1fecb9"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst = item_Day_fcst.merge(store_item, on= 'item')\n",
    "\n",
    "item_Day_fcst['sales'] = item_Day_fcst['Prediction'] * item_Day_fcst['pct']/100\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "3d335f8241ae285cd3e6558dccd1dbb49f161e82"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst = item_Day_fcst.loc[:,['Week #','Day','store','item','sales']]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b72e7ed6eb4654d0b45e893050bd71452a8c8ea"
   },
   "source": [
    "#### Convert Week Number and Day of Week into Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "6bd69d7c57c3648b0e1914b37b2f12820578f61c"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst['Date'] = item_Day_fcst.apply(lambda row: str_to_date(row), axis=1)\n",
    "item_Day_fcst.index = item_Day_fcst['Date']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "feaa2f510a6ba8f17a0662f36af641e6d3d69207"
   },
   "outputs": [],
   "source": [
    "\"\"\"item_Day_fcst.sort_values(['item','store','Date'], inplace=True)\n",
    "item_Day_fcst['sales']= round(item_Day_fcst['sales'], 0)\n",
    "\n",
    "item_Day_fcst = item_Day_fcst[['store','item','sales']].loc[\n",
    "    item_Day_fcst.index < pd.to_datetime('January 1, 2018')]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb65b639b08f28b219875de000aec82ee382fd9b"
   },
   "source": [
    "#### Model Accuracy\nWe get 19.60% error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "56ec5b08998e6c8f372caed035f3314f5605544f"
   },
   "outputs": [],
   "source": [
    "\"\"\"SMAPE(item_Day_fcst['sales'], test['sales'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef208f7cba34ef824d404973ed9b05868fe6a710"
   },
   "source": [
    "# DNN Model\n## Model (2) - Feed Forward Neural Network with Daily Data\n\nTo really take advantage of the DNN, we need to add features. We won't be adding any rolling/ expanding windows since they'd be unreliable on the competition data. Most of the engineered features are categorical, with the exception being prior year sales.\n\nBelow are some constants we'll need to use for working with datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "59b44005b2ca4231d7d34a799c025fe135ba8d10"
   },
   "outputs": [],
   "source": [
    "ns_per_day = 86400000000000\n",
    "start_date = pd.to_datetime('January 1, 2013')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e0340e68d4dfdff5d5beed133cf108ceab4bf23"
   },
   "source": [
    "### Feature Engineering\n**Day of Week**\n + Utilizing pandas builtin dayofweek call.\n\n**Month**\n + Utilizing pandas buitlin month call.\n \n**Day of Year**\n + Take the number of days since the data started, then take (mod 364) for a like-for-like day of year.\n + Want to do this instead of calendar day of year because the dates land on different days of the week. Example: January 1 might be a Tuesday one year so it'll be a Wednesday next year.\n \n**Year**\n + Take the number of days since the data started, then take the quotient when divided by 364 and subtract 1. The subtraction is so we can easily remove the first year of data as there is no prior year data.\n + This is to give us similar years, same reasoning as above.\n \n**Is Weekend**\n + Boolean value if the date falls on a weekend. This is because a majority of sales occur between Friday and Sunday.\n \n**Item Quart**\n + Which quartile trend the item most closely resembles. This comes from the dynamic time warping we had done for the ARIMA models.\n \n**12 Month Lag**\n + Prior year's sales (same store, same item, 364 days prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "ca0c101b7ac7e908e7ab9731e4f13aad947990ee"
   },
   "outputs": [],
   "source": [
    "itm_quart = pd.DataFrame(fitted_items_WK, columns=['item','item_quart','item_metric'])\n",
    "\n",
    "def add_feat(df):\n",
    "    \"\"\"Takes DataFrame and returns DataFrame with added features\"\"\"\n",
    "\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    df['Month'] = df.index.month\n",
    "    df['Day_of_Year'] = ((df.index - start_date)/ ns_per_day).astype(int) % 364\n",
    "    df['Year'] = ((df.index - start_date)/ ns_per_day).astype(int) // 364 -1\n",
    "    df['is_wknd'] = df['Day_of_Week'] // 4 # Fri-Sun are 4-6, Monday is 0 so this works\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Add item quartile as feature\n",
    "    df = df.merge(itm_quart, on='item').drop('item_metric', axis=1)\n",
    "\n",
    "    # Add prior year sales as additional column/ feature\n",
    "    prior_year_sales = df[['date','sales','store','item']]\n",
    "    prior_year_sales['date'] += pd.Timedelta('364 days')\n",
    "    prior_year_sales.columns =['date','lag_12mo','store','item']\n",
    "\n",
    "    df = df.merge(prior_year_sales, on=['date','store','item'])\n",
    "    \n",
    "    # Remove first year of data as there is no prior year sales for them\n",
    "    df['store'] -=1\n",
    "    df['item'] -=1\n",
    "    df = df[df['Year'] >=0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "9bcb90dc3d315500815a4d64d2dbdb366f1df793"
   },
   "outputs": [],
   "source": [
    "df = add_feat(df)\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "68e6d8b841d5a58b01ce59e9ef7b4dee6345dd24"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "974e424553cb4da6788b03c363abda3299e891a1"
   },
   "source": [
    "### Train & Test Data Split\nSplit train and test data by setting the last 91 days (everything after October 3, 2017) as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "a01e52bbaf58f3df29b917de217106bd1a3a8150"
   },
   "outputs": [],
   "source": [
    "x_train = df.loc[df['date'] < pd.to_datetime('October 3, 2017')].drop(['sales','date','Day'], axis=1)\n",
    "y_train = df.loc[df['date'] < pd.to_datetime('October 3, 2017'), 'sales']\n",
    "\n",
    "x_test = df.loc[df['date'] >= pd.to_datetime('October 3, 2017')].drop(['sales','date','Day'], axis=1).reset_index(drop=True)\n",
    "y_test = df.loc[df['date'] >= pd.to_datetime('October 3, 2017'), 'sales'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be9653ea6abc94200df6245c61b9c4efcb177c47"
   },
   "source": [
    "### Feature Columns\nSetup the feature colunms in the tensorflow model. Most of the features are categorical, the only numeric one is 'lag_12mo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "03fdd64c37f0f12b808183f4ab664814ffcd69cc"
   },
   "outputs": [],
   "source": [
    "feat_cols =[]\n",
    "\n",
    "for col in x_train.drop('lag_12mo', axis=1).columns:\n",
    "    feat_cols.append(tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_identity(col, max(df[col])+1),1))\n",
    "    \n",
    "feat_cols.append(tf.feature_column.numeric_column(key='lag_12mo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8a7a00ef8a3d8ed184ac704a66995beea91867a"
   },
   "source": [
    "### Training the Model & Forecasting\nSetup the training (input) function in tensorflow. Sending 6 months (180 days) of data to train on at once and will run through the entire dataset 80 times. We won't shuffle the observations for this exercise. Idea being that the order of observations matters since this is a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "b1fb5c588c15d70da49cbd8f56fefb7e94977a3e"
   },
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x= x_train, y= y_train, batch_size= 180, num_epochs= 80,\n",
    "                                                 shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6c2c6552996af976ac33b0a5abff8a385fc26d7"
   },
   "source": [
    "The model we'll use is tensorflow's builtin DNNRegressor with 3 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "1921e8cca876ffbf0cc8243e304e079cad4eab05"
   },
   "outputs": [],
   "source": [
    "regressor = tf.estimator.DNNRegressor(hidden_units= [20, 10, 20], feature_columns= feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "d1cf73a5761ce4391f3e54bfb5afd87867be28ad"
   },
   "outputs": [],
   "source": [
    "regressor.train(input_fn= input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "762e5c8513056c51e07385ae8b5b836827d0d595"
   },
   "outputs": [],
   "source": [
    "pred_fn = tf.estimator.inputs.pandas_input_fn(x= x_test, batch_size =len(x_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "a870573e0ccccc0e32df31c1aa790f56f422f142"
   },
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "c690bbcc16df34c9cc8d70ba55e6994e13547896"
   },
   "outputs": [],
   "source": [
    "predictions = list(regressor.predict(input_fn= pred_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aaaa171ae5e244379c4433dbc94cadb93778fec3"
   },
   "source": [
    "### Model Performance\nGather predictions into a series then use SMAPE to compare with actual test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "52ac58f1ed2e8251c9131c8bf510dece9603bd9e"
   },
   "outputs": [],
   "source": [
    "final_pred = []\n",
    "\n",
    "for pred in predictions:\n",
    "    final_pred.append(pred['predictions'][0])\n",
    "\n",
    "final_pred = pd.DataFrame(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "4a283cc33acf6e6d3f83e5fe9423ed468b4c9d52"
   },
   "outputs": [],
   "source": [
    "SMAPE(final_pred.iloc[:,0], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "869413dffbdbc544c5cdda62c01dfd3bfef9f19b"
   },
   "source": [
    "# XGBoost Model \n## Model (3) - Extreme Gradient Boost with Daily Data\nAccording to the competition description, this model should provide the best accuracy. Let's feed it the same data as the DNN and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "9061699d5d04fde7a3a9f8e4fb85be6d6cadc8d5"
   },
   "outputs": [],
   "source": [
    "preds = xboost(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e58d69a38b9fe7eee94fb9155c1fd0c7eefee14b"
   },
   "source": [
    "### Model Performance\nCompare forecasts to the actual test sales using SMAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e005088f9db8389eb41500a57a3d86e0bc846f34"
   },
   "outputs": [],
   "source": [
    "SMAPE(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0890dc7356f14c39d7204f53204c2563da52c5ce"
   },
   "source": [
    "# My Learnings\n\n## ARIMA\nClustering the items to the nearest quartile keeps roughly the same accuracy as not clustering while taking less time to forecast. The models provided quick results although the least accurate of those tested.\n\n## DNN\nLikely due to the amount of data and how many times the model ran through all the data, the training sessions took a reltaively long time to run. However there was an accuracy boost compared to the ARIMA models. A different neural network structure (i.e. a deep and wide net) could possibly provide even better results.\n\n## XGBoost\nThis model is a beast. It didn't take very long to train and was the most accurate model of the 4 tested. Can see why this has won so many competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b13a0dac3ad3dacdfe2fd991e6b5ab43b4ff597"
   },
   "source": [
    "# Competition Submission\n\nUsing the same xgboost model with same feature engineering. This time we'll use the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "75b77ba96fa619987d38ec8374cc85de9e8175b8"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path + 'train.csv', index_col=0)\n",
    "df2 = pd.read_csv(path + 'test.csv', index_col=1)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "0b16feb5f9101fcc3495e3d0e9edb247f2f2b8be"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2])\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "0375d383e99251039d770bb1c1e2e9008e765530"
   },
   "outputs": [],
   "source": [
    "df = add_feat(df)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "a86b36e9ec09c1d97b4bb550b00605123af541b1"
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "42be6e71b7ff603a732ae65a09865f75ef6c3b2b"
   },
   "outputs": [],
   "source": [
    "x_train = df[pd.isnull(df['id'])].drop(['id','sales','date'], axis=1)\n",
    "y_train = df[pd.isnull(df['id'])]['sales']\n",
    "\n",
    "x_test = df[pd.notnull(df['id'])].drop(['id','sales','date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "944594fa02f5a8fe08cf93ebfe613cf6277e0b0e"
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(xboost(x_train, y_train, x_test)).reset_index()\n",
    "preds.columns =['id','sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "781ce452086381fdef4e255abba879928ba0152b"
   },
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false,
    "_uuid": "e0294696a12ee36a66f9f49da992dfb5f17c90e5"
   },
   "outputs": [],
   "source": [
    "preds.to_csv('sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6701b5b88352e3611575230d4957fff623d1d836"
   },
   "source": [
    "# Thanks for Making It to the End!\nThank you for sharing in my first competition! Hopefully you learned something as well. As this is my first competition and kernel, any feedback would be greatly appreciated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
